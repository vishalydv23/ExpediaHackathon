{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This notebook is inspired from https://www.kaggle.com/fabiendaniel/customer-segmentation\n",
    "\n",
    "With our data ready to be clustered from the notebook HackathonEDA. We can start clustering the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load the latest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "data = pd.read_csv(\"../../data/processed/tempDataFrameLandmark/ratefinalData2ndCheckpoint.csv\")\n",
    "data = data.drop(['Unnamed: 0'], axis = 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating clusters of counties with high number of tourist per population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I will group the counties based on how popular they are in tourism considering their overall population. We will start with good old <b>kmeans</b> clustering. But before that, lets create a new dataframe that contains the values of tourism out of the complete data that has been prepared. We are interested in following fields. \n",
    "\n",
    "* lodgingInventoryBucketNonVacationalRental\n",
    "* lodgingInventoryBucketVacationalRental\n",
    "* lIBNonVRRatioToPopulation\n",
    "* lIBVRRatioToPopulation\n",
    "* aIPBRatioToPopulation\n",
    "* AERAFEmployementRatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourismByCounty = data.filter(['lodgingInventoryBucketNonVacationalRental', 'lodgingInventoryBucketVacationalRental', 'lIBNonVRRatioToPopulation', 'lIBVRRatioToPopulation', 'aIPBRatioToPopulation', 'AERAFEmployementRatio'], axis=1)\n",
    "tourismByCounty.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to define (approximately) the number of clusters that best represents the data, I use the <b>silhouette score</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "matrix = tourismByCounty.as_matrix()\n",
    "for n_clusters in tqdm(range(3,10)):\n",
    "    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30)\n",
    "    kmeans.fit(matrix)\n",
    "    clusters = kmeans.predict(matrix)\n",
    "    silhouette_avg = silhouette_score(matrix, clusters)\n",
    "    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)\n",
    "    if(n_clusters % 10 == 0):\n",
    "        unique, counts = np.unique(clusters, return_counts=True)\n",
    "        print(\"Minimum values in cluster is: \" , min(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score for 3 clusters is best but this will be very less of a cluster. Let's group the values in 5 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_clusters = 5\n",
    "silhouette_avg = -1\n",
    "while silhouette_avg < 0.3199:\n",
    "    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30)\n",
    "    kmeans.fit(matrix)\n",
    "    clusters = kmeans.predict(matrix)\n",
    "    silhouette_avg = silhouette_score(matrix, clusters)\n",
    "    \n",
    "    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Characterizing the content of clusters</b>\n",
    "\n",
    "Number of elements in each clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "unique, counts = np.unique(clusters, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Silhouette intra-cluster score</b>\n",
    "\n",
    "In order to have an insight on the quality of the classification, we can represent the silhouette scores of each element of the different clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_component_silhouette(n_clusters, lim_x, mat_size, sample_silhouette_values, clusters):\n",
    "    plt.rcParams[\"patch.force_edgecolor\"] = True\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    mpl.rc('patch', edgecolor = 'dimgray', linewidth=1)\n",
    "    #____________________________\n",
    "    fig, ax1 = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(10, 8)\n",
    "    ax1.set_xlim([lim_x[0], lim_x[1]])\n",
    "    ax1.set_ylim([0, mat_size + (n_clusters + 1) * 10])\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        #___________________________________________________________________________________\n",
    "        # Aggregate the silhouette scores for samples belonging to cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[clusters == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        cmap = cm.get_cmap(\"Spectral\")\n",
    "        color = cmap(float(i) / n_clusters)        \n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values,\n",
    "                           facecolor=color, edgecolor=color, alpha=0.8)\n",
    "        #____________________________________________________________________\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.03, y_lower + 0.5 * size_cluster_i, str(i), color = 'red', fontweight = 'bold',\n",
    "                bbox=dict(facecolor='white', edgecolor='black', boxstyle='round, pad=0.3'))\n",
    "        #______________________________________\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#____________________________________\n",
    "# define individual silouhette scores\n",
    "sample_silhouette_values = silhouette_samples(matrix, clusters)\n",
    "#__________________\n",
    "# and do the graph\n",
    "graph_component_silhouette(n_clusters, [-0.4, 0.7], len(tourismByCounty), sample_silhouette_values, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a look at what each cluster and what is in them. And Rank clusters which attract more tourist per population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "clusterScore = {}\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    county_cluster = tourismByCounty.loc[clusters == i]\n",
    "    clusterScore[i] = (county_cluster['lIBNonVRRatioToPopulation'].mean() + \n",
    "                           county_cluster['lIBVRRatioToPopulation'].mean() + \n",
    "                           county_cluster['aIPBRatioToPopulation'].mean() + \n",
    "                           county_cluster['AERAFEmployementRatio'].mean()) / len(county_cluster)\n",
    "    \n",
    "sorted_d = dict( sorted(clusterScore.items(), key=operator.itemgetter(1),reverse=True))\n",
    "print('Dictionary in descending order by value : ',sorted_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the cluster number to the counties in main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.loc[:, 'tourismCluster'] = clusters\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first county with highest tourism per population i.e counties in cluster 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['tourismCluster'] == 4].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the county: Denali Borough in Alaska. And Denali National Park is Alaskaâ€™s most popular land attraction. Source: https://www.alaska.org/destination/denali-national-park\n",
    "\n",
    "Which proves that clustering done by our algorithm is quite nice. Here is a image of this beautiful county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"../../data/raw/Images/denali.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r'../../data/processed/tempDataFrameLandmark/dataWithTourismCluster.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering of counties based on positive Economic social impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will cluster the counties that are bringing positive economic impact to the society. Following are the fields that we are going to use to do clustering.\n",
    "* medianHouseHoldIncome\n",
    "* unEmployementRate\n",
    "* vacantHousingUnitsRatio\n",
    "* familiesUnderPovertyScale\n",
    "* totalEmployedInOwnBusinesRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling median household income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['medianHouseHoldIncome'] = (data['medianHouseHoldIncome'] / max(data['medianHouseHoldIncome'])) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "economicsByCounties = data.filter(['medianHouseHoldIncome', 'unEmployementRate', 'vacantHousingUnitsRatio', 'familiesUnderPovertyScale', 'totalEmployedInOwnBusinesRate'], axis=1)\n",
    "economicsByCounties.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data encoding</b>\n",
    "The different variables I selected have quite different ranges of variation and before continuing the analysis, I create a matrix where these data are standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "matrix = economicsByCounties.as_matrix()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(matrix)\n",
    "print('variables mean values: \\n' + 90*'-' + '\\n' , scaler.mean_)\n",
    "scaled_matrix = scaler.transform(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# matrix = economicsByCounties.as_matrix()\n",
    "for n_clusters in tqdm(range(3,10)):\n",
    "    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30)\n",
    "    kmeans.fit(scaled_matrix)\n",
    "    clusters = kmeans.predict(scaled_matrix)\n",
    "    silhouette_avg = silhouette_score(scaled_matrix, clusters)\n",
    "    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)\n",
    "    if(n_clusters % 10 == 0):\n",
    "        unique, counts = np.unique(clusters, return_counts=True)\n",
    "        print(\"Minimum values in cluster is: \" , min(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a jump till cluster 6 and then the silhouette score starts to drop. So lets take 6 as the cluster size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 5\n",
    "silhouette_avg = -1\n",
    "while silhouette_avg < 0.255:\n",
    "    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30)\n",
    "    kmeans.fit(scaled_matrix)\n",
    "    clusters = kmeans.predict(scaled_matrix)\n",
    "    silhouette_avg = silhouette_score(scaled_matrix, clusters)\n",
    "    \n",
    "    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Characterizing the content of clusters</b>\n",
    "\n",
    "Number of elements in each clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(clusters, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Silhouette intra-cluster score</b>\n",
    "\n",
    "In order to have an insight on the quality of the classification, we can represent the silhouette scores of each element of the different clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_silhouette_values = silhouette_samples(scaled_matrix, clusters)\n",
    "#____________________________________\n",
    "# define individual silouhette scores\n",
    "sample_silhouette_values = silhouette_samples(scaled_matrix, clusters)\n",
    "#__________________\n",
    "# and do the graph\n",
    "graph_component_silhouette(n_clusters, [-0.2, 0.6], len(scaled_matrix), sample_silhouette_values, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Counties Morphotype</b>\n",
    "I have verified that the different clusters are indeed disjoint (at least, in a global way). It remains to understand the behaviour of the counties in each cluster. To do so, I start by adding to the economicsByCounties dataframe a variable that defines the cluster to which each client belongs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "economicsByCounties.loc[:, 'cluster'] = clusters\n",
    "economicsByCounties.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.DataFrame()\n",
    "for i in range(n_clusters):\n",
    "    test = pd.DataFrame(economicsByCounties[economicsByCounties['cluster'] == i].mean())\n",
    "    test = test.T.set_index('cluster', drop = True)\n",
    "    test['size'] = economicsByCounties[economicsByCounties['cluster'] == i].shape[0]\n",
    "    merged_df = pd.concat([merged_df, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _scale_data(data, ranges):\n",
    "    (x1, x2) = ranges[0]\n",
    "    d = data[0]\n",
    "    return [(d - y1) / (y2 - y1) * (x2 - x1) + x1 for d, (y1, y2) in zip(data, ranges)]\n",
    "\n",
    "class RadarChart():\n",
    "    def __init__(self, fig, location, sizes, variables, ranges, n_ordinate_levels = 6):\n",
    "\n",
    "        angles = np.arange(0, 360, 360./len(variables))\n",
    "\n",
    "        ix, iy = location[:] ; size_x, size_y = sizes[:]\n",
    "        \n",
    "        axes = [fig.add_axes([ix, iy, size_x, size_y], polar = True, \n",
    "        label = \"axes{}\".format(i)) for i in range(len(variables))]\n",
    "\n",
    "        _, text = axes[0].set_thetagrids(angles, labels = variables)\n",
    "        \n",
    "        for txt, angle in zip(text, angles):\n",
    "            if angle > -1 and angle < 181:\n",
    "                txt.set_rotation(angle - 90)\n",
    "            else:\n",
    "                txt.set_rotation(angle - 270)\n",
    "        \n",
    "        for ax in axes[1:]:\n",
    "            ax.patch.set_visible(False)\n",
    "            ax.xaxis.set_visible(False)\n",
    "            ax.grid(\"off\")\n",
    "        \n",
    "        for i, ax in enumerate(axes):\n",
    "            grid = np.linspace(*ranges[i],num = n_ordinate_levels)\n",
    "            grid_label = [\"\"]+[\"{:.0f}\".format(x) for x in grid[1:-1]]\n",
    "            ax.set_rgrids(grid, labels = grid_label, angle = angles[i])\n",
    "            ax.set_ylim(*ranges[i])\n",
    "        \n",
    "        self.angle = np.deg2rad(np.r_[angles, angles[0]])\n",
    "        self.ranges = ranges\n",
    "        self.ax = axes[0]\n",
    "                \n",
    "    def plot(self, data, *args, **kw):\n",
    "        sdata = _scale_data(data, self.ranges)\n",
    "        self.ax.plot(self.angle, np.r_[sdata, sdata[0]], *args, **kw)\n",
    "\n",
    "    def fill(self, data, *args, **kw):\n",
    "        sdata = _scale_data(data, self.ranges)\n",
    "        self.ax.fill(self.angle, np.r_[sdata, sdata[0]], *args, **kw)\n",
    "\n",
    "    def legend(self, *args, **kw):\n",
    "        self.ax.legend(*args, **kw)\n",
    "        \n",
    "    def title(self, title, *args, **kw):\n",
    "        self.ax.text(0.9, 1, title, transform = self.ax.transAxes, *args, **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,12))\n",
    "\n",
    "attributes = ['medianHouseHoldIncome', 'unEmployementRate', 'vacantHousingUnitsRatio', 'familiesUnderPovertyScale', 'totalEmployedInOwnBusinesRate']\n",
    "ranges = [[0.01, 100], [0.01, 100], [0.01, 100], [0.01, 100], [0.01, 100]]\n",
    "index  = [0, 1, 2, 3, 4]\n",
    "\n",
    "n_groups = n_clusters ; i_cols = 2\n",
    "i_rows = n_groups//i_cols\n",
    "size_x, size_y = (1/i_cols), (1/i_rows)\n",
    "\n",
    "for ind in range(n_clusters):\n",
    "    ix = ind%3 ; iy = i_rows - ind//3\n",
    "    pos_x = ix*(size_x + 0.05) ; pos_y = iy*(size_y + 0.05)            \n",
    "    location = [pos_x, pos_y]  ; sizes = [size_x, size_y] \n",
    "    #______________________________________________________\n",
    "    values = np.array(merged_df.loc[index[ind], attributes])    \n",
    "    radar = RadarChart(fig, location, sizes, attributes, ranges)\n",
    "    radar.plot(values, color = 'b', linewidth=2.0)\n",
    "    radar.fill(values, alpha = 0.2, color = 'b')\n",
    "    radar.title(title = 'cluster nÂº{}'.format(index[ind]), color = 'r')\n",
    "    ind += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a look at what each cluster and what is in them. And Rank clusters which attract more tourist per population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterScore = {}\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    county_cluster = economicsByCounties.loc[clusters == i]\n",
    "    clusterScore[i] = (county_cluster['medianHouseHoldIncome'].mean() - \n",
    "                           county_cluster['unEmployementRate'].mean() - \n",
    "                           county_cluster['vacantHousingUnitsRatio'].mean() - \n",
    "                           county_cluster['familiesUnderPovertyScale'].mean() + \n",
    "                           county_cluster['totalEmployedInOwnBusinesRate'].mean()) / len(county_cluster)\n",
    "    \n",
    "sorted_d = dict( sorted(clusterScore.items(), key=operator.itemgetter(1),reverse=True))\n",
    "print('Dictionary in descending order by value : ',sorted_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the value  to the main dataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:,'economicsCluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r'../../data/processed/tempDataFrameLandmark/dataWithEconomicCluster.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering of counties based on positive Cultural social impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will cluster the counties that are bringing positive cultural impact to the society. Following are the fields that we are going to use to do clustering.\n",
    "* minorityPopulationRatio\n",
    "* structureBuiltYearBefore1939Ratio\n",
    "* CustomerSatisfactionAvgReviewRating\n",
    "* CustomerSatisfactionAvgStarRating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cultureByCounties = data.filter(['minorityPopulationRatio', 'structureBuiltYearBefore1939Ratio', 'CustomerSatisfactionAvgReviewRating', 'CustomerSatisfactionAvgStarRating'], axis=1)\n",
    "cultureByCounties.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data encoding</b>\n",
    "The different variables I selected have quite different ranges of variation and before continuing the analysis, I create a matrix where these data are standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = cultureByCounties.as_matrix()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(matrix)\n",
    "print('variables mean values: \\n' + 90*'-' + '\\n' , scaler.mean_)\n",
    "scaled_matrix = scaler.transform(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# matrix = economicsByCounties.as_matrix()\n",
    "for n_clusters in tqdm(range(3,10)):\n",
    "    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30)\n",
    "    kmeans.fit(scaled_matrix)\n",
    "    clusters = kmeans.predict(scaled_matrix)\n",
    "    silhouette_avg = silhouette_score(scaled_matrix, clusters)\n",
    "    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)\n",
    "    if(n_clusters % 10 == 0):\n",
    "        unique, counts = np.unique(clusters, return_counts=True)\n",
    "        print(\"Minimum values in cluster is: \" , min(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a jump till cluster 4 and then the silhouette score starts to drop. So lets take 4 as the cluster size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_clusters = 4\n",
    "silhouette_avg = -1\n",
    "while silhouette_avg < 0.261:\n",
    "    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30)\n",
    "    kmeans.fit(scaled_matrix)\n",
    "    clusters = kmeans.predict(scaled_matrix)\n",
    "    silhouette_avg = silhouette_score(scaled_matrix, clusters)\n",
    "    \n",
    "    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Characterizing the content of clusters</b>\n",
    "\n",
    "Number of elements in each clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(clusters, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Silhouette intra-cluster score</b>\n",
    "\n",
    "In order to have an insight on the quality of the classification, we can represent the silhouette scores of each element of the different clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_silhouette_values = silhouette_samples(scaled_matrix, clusters)\n",
    "#____________________________________\n",
    "# define individual silouhette scores\n",
    "sample_silhouette_values = silhouette_samples(scaled_matrix, clusters)\n",
    "#__________________\n",
    "# and do the graph\n",
    "graph_component_silhouette(n_clusters, [-0.15, 0.55], len(scaled_matrix), sample_silhouette_values, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Counties Morphology</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cultureByCounties.loc[:, 'cluster'] = clusters\n",
    "cultureByCounties.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.DataFrame()\n",
    "for i in range(n_clusters):\n",
    "    test = pd.DataFrame(cultureByCounties[cultureByCounties['cluster'] == i].mean())\n",
    "    test = test.T.set_index('cluster', drop = True)\n",
    "    test['size'] = cultureByCounties[cultureByCounties['cluster'] == i].shape[0]\n",
    "    merged_df = pd.concat([merged_df, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,12))\n",
    "\n",
    "attributes = ['minorityPopulationRatio', 'structureBuiltYearBefore1939Ratio', 'CustomerSatisfactionAvgReviewRating', 'CustomerSatisfactionAvgStarRating']\n",
    "ranges = [[0.01, 100], [0.01, 100], [0.01, 100], [0.01, 20], [0.01, 20]]\n",
    "index  = [0, 1, 2, 3]\n",
    "\n",
    "n_groups = n_clusters ; i_cols = 2\n",
    "i_rows = n_groups//i_cols\n",
    "size_x, size_y = (1/i_cols), (1/i_rows)\n",
    "\n",
    "for ind in range(n_clusters):\n",
    "    ix = ind%3 ; iy = i_rows - ind//3\n",
    "    pos_x = ix*(size_x + 0.05) ; pos_y = iy*(size_y + 0.05)            \n",
    "    location = [pos_x, pos_y]  ; sizes = [size_x, size_y] \n",
    "    #______________________________________________________\n",
    "    values = np.array(merged_df.loc[index[ind], attributes])    \n",
    "    radar = RadarChart(fig, location, sizes, attributes, ranges)\n",
    "    radar.plot(values, color = 'b', linewidth=2.0)\n",
    "    radar.fill(values, alpha = 0.2, color = 'b')\n",
    "    radar.title(title = 'cluster nÂº{}'.format(index[ind]), color = 'r')\n",
    "    ind += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a look at what each cluster and what is in them. And Rank clusters which attract more tourist per population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterScore = {}\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    county_cluster = cultureByCounties.loc[clusters == i]\n",
    "    clusterScore[i] = (county_cluster['minorityPopulationRatio'].mean() +\n",
    "                           county_cluster['structureBuiltYearBefore1939Ratio'].mean() + \n",
    "                           county_cluster['CustomerSatisfactionAvgReviewRating'].mean() + \n",
    "                           county_cluster['CustomerSatisfactionAvgStarRating'].mean()) / len(county_cluster)\n",
    "    \n",
    "sorted_d = dict( sorted(clusterScore.items(), key=operator.itemgetter(1),reverse=True))\n",
    "print('Dictionary in descending order by value : ',sorted_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the value  to the main dataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:,'cultureCluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r'../../data/processed/tempDataFrameLandmark/dataWithCultureCluster.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering of counties based on positive Environemtal social impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will cluster the counties that are bringing positive Environmental impact to the society. Following are the fields that we are going to use to do clustering.\n",
    "* waterUsage\n",
    "* bicycleUsageRate\n",
    "* airQulaityPM2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environmentByCounties = data.filter(['waterUsage', 'bicycleUsageRate', 'airQulaityPM2.5'], axis=1)\n",
    "environmentByCounties.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data encoding</b>\n",
    "The different variables I selected have quite different ranges of variation and before continuing the analysis, I create a matrix where these data are standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "matrix = environmentByCounties.as_matrix()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(matrix)\n",
    "print('variables mean values: \\n' + 90*'-' + '\\n' , scaler.mean_)\n",
    "scaled_matrix = scaler.transform(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix = economicsByCounties.as_matrix()\n",
    "for n_clusters in tqdm(range(3,10)):\n",
    "    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30)\n",
    "    kmeans.fit(scaled_matrix)\n",
    "    clusters = kmeans.predict(scaled_matrix)\n",
    "    silhouette_avg = silhouette_score(scaled_matrix, clusters)\n",
    "    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)\n",
    "    if(n_clusters % 10 == 0):\n",
    "        unique, counts = np.unique(clusters, return_counts=True)\n",
    "        print(\"Minimum values in cluster is: \" , min(counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a jump till cluster 6 and then the silhouette score starts to drop. So lets take 6 as the cluster size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 6\n",
    "silhouette_avg = -1\n",
    "while silhouette_avg < 0.4786:\n",
    "    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30)\n",
    "    kmeans.fit(scaled_matrix)\n",
    "    clusters = kmeans.predict(scaled_matrix)\n",
    "    silhouette_avg = silhouette_score(scaled_matrix, clusters)\n",
    "    \n",
    "    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Characterizing the content of clusters</b>\n",
    "\n",
    "Number of elements in each clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(clusters, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Silhouette intra-cluster score</b>\n",
    "\n",
    "In order to have an insight on the quality of the classification, we can represent the silhouette scores of each element of the different clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_silhouette_values = silhouette_samples(scaled_matrix, clusters)\n",
    "#____________________________________\n",
    "# define individual silouhette scores\n",
    "sample_silhouette_values = silhouette_samples(scaled_matrix, clusters)\n",
    "#__________________\n",
    "# and do the graph\n",
    "graph_component_silhouette(n_clusters, [-0.2, 0.8], len(scaled_matrix), sample_silhouette_values, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Counties Morphology</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environmentByCounties.loc[:, 'cluster'] = clusters\n",
    "environmentByCounties.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.DataFrame()\n",
    "for i in range(n_clusters):\n",
    "    test = pd.DataFrame(environmentByCounties[environmentByCounties['cluster'] == i].mean())\n",
    "    test = test.T.set_index('cluster', drop = True)\n",
    "    test['size'] = environmentByCounties[environmentByCounties['cluster'] == i].shape[0]\n",
    "    merged_df = pd.concat([merged_df, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,12))\n",
    "\n",
    "attributes = ['waterUsage', 'bicycleUsageRate', 'airQulaityPM2.5']\n",
    "ranges = [[0.01, 100], [0.01, 100], [0.01, 100]]\n",
    "index  = [0, 1, 2]\n",
    "\n",
    "n_groups = n_clusters ; i_cols = 2\n",
    "i_rows = n_groups//i_cols\n",
    "size_x, size_y = (1/i_cols), (1/i_rows)\n",
    "\n",
    "for ind in range(n_clusters):\n",
    "    ix = ind%3 ; iy = i_rows - ind//3\n",
    "    pos_x = ix*(size_x + 0.05) ; pos_y = iy*(size_y + 0.05)            \n",
    "    location = [pos_x, pos_y]  ; sizes = [size_x, size_y] \n",
    "    #______________________________________________________\n",
    "    values = np.array(merged_df.loc[index[ind], attributes])    \n",
    "    radar = RadarChart(fig, location, sizes, attributes, ranges)\n",
    "    radar.plot(values, color = 'b', linewidth=2.0)\n",
    "    radar.fill(values, alpha = 0.2, color = 'b')\n",
    "    radar.title(title = 'cluster nÂº{}'.format(index[ind]), color = 'r')\n",
    "    ind += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take a look at what each cluster and what is in them. And Rank clusters which attract more tourist per population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterScore = {}\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    county_cluster = environmentByCounties.loc[clusters == i]\n",
    "    clusterScore[i] = (county_cluster['bicycleUsageRate'].mean() -\n",
    "                       county_cluster['waterUsage'].mean() - \n",
    "                       county_cluster['airQulaityPM2.5'].mean()) / len(county_cluster)\n",
    "    \n",
    "sorted_d = dict( sorted(clusterScore.items(), key=operator.itemgetter(1),reverse=True))\n",
    "print('Dictionary in descending order by value : ',sorted_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the value  to the main dataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:,'environmentCluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r'../../data/processed/tempDataFrameLandmark/dataWithEnvironmentCluster.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
